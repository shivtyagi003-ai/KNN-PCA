{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "- K-Nearest Neighbors (KNN) is a simple, intuitive, and non-parametric supervised learning algorithm. Its core philosophy is \"similarity\": it assumes that similar data points exist in close proximity to each other.\n",
        "How KNN works?\n",
        "1- Choose the value of K: Decide how many \"neighbors\" (nearby data points) the algorithm should look at.\n",
        "2- Calculate Distance: When a new data point (query point) arrives, the algorithm calculates its distance from every single point in the training set.\n",
        "3- Find the K Nearest Neighbors: Sort the calculated distances and pick the K points with the smallest values.\n",
        "\n",
        "2. What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "- The Curse of Dimensionality refers to a set of phenomena that occur when analyzing and organizing data in high-dimensional spaces (datasets with a large number of features) that do not happen in low-dimensional settings.\n",
        "\n",
        "How it Affects KNN Performance?\n",
        "1- Distance Concentration (Distances Lose Meaning)\n",
        "In high-dimensional space, the difference between the distance to the nearest neighbor and the distance to the farthest neighbor tends to become negligible.\n",
        "2- Data Sparsity\n",
        "To maintain the same density of data points as you add dimensions, you would need an exponential increase in the amount of data.\n",
        "3- Increased Computational Complexity\n",
        "KNN is already a \"lazy learner\" that performs calculations at the time of prediction.\n",
        "\n",
        "3. What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "- Principal Component Analysis (PCA) is a powerful statistical technique used to simplify complex datasets. It is the most popular method for dimensionality reduction, transforming a large set of variables into a smaller one that still contains most of the information from the original set.\n",
        "How PCA different from feature selection?\n",
        "1- Fundamental Definition:-\n",
        "Feature Selection: This is a process of choosing. You select a subset of the original variables and discard the rest. The features you keep remain exactly as they were.\n",
        "\n",
        "PCA (Feature Extraction): This is a process of transformation. It combines the original variables into a set of entirely new, artificial variables (Principal Components).\n",
        "\n",
        "2- Information Retention:-\n",
        "Feature Selection: Information in the discarded features is completely lost. If you drop \"Year of Birth\" because it's redundant with \"Age,\" that specific column is gone.\n",
        "\n",
        "PCA: It attempts to compress information. Even if you reduce 10 variables down to 3 Principal Components, those 3 components still contain \"bits\" of information from all 10 original variables.\n",
        "\n",
        "3- Mathematical Relationship:-\n",
        "Feature Selection: Features are evaluated based on their relationship to the target variable (e.g., how well \"Price\" correlates with \"Sales\").\n",
        "\n",
        "PCA: Components are created based on variance within the features themselves. It is an unsupervised technique that doesn't care about the target variable; it only looks for where the most \"spread\" exists in the data.\n",
        "\n",
        "4. What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "- In Principal Component Analysis (PCA), eigenvalues and eigenvectors are the mathematical engines that allow the algorithm to actually reduce dimensionality while keeping your data's \"soul\" (its variance) intact.\n",
        "How are they important:-\n",
        "They allow PCA to perform three critical tasks:\n",
        "Ranking Information: By sorting eigenvalues from largest to smallest, PCA automatically ranks your new features by how much \"information\" they provide.\n",
        "\n",
        "Dimensionality Reduction: You can decide to keep only the eigenvectors with the largest eigenvalues  and safely discard the rest. This shrinks your dataset while retaining most of its patterns.\n",
        "\n",
        "Eliminating Correlation: The eigenvectors are mathematically guaranteed to be perpendicular to each other. This means your new features (Principal Components) are uncorrelated, which solves the problem of \"redundant\" data.\n",
        "\n",
        "5.  How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "- When applied in a single machine learning pipeline, PCA and KNN act as a powerful duo where the strengths of one directly address the weaknesses of the other. PCA handles the data preparation and structural cleanup, while KNN focuses on the final decision-making.\n",
        "Here is how they complement each other point-by-point:\n",
        "1- Defeating the \"Curse of Dimensionality-\n",
        "The Complement: PCA shrinks the \"vast vacuum\" of high-dimensional space into a much smaller, denser subspace. By reducing 100 features down to 5 principal components, PCA ensures that the \"nearest\" neighbors KNN finds are actually relevant and truly similar.\n",
        "\n",
        "2- Radical Speed Improvements-\n",
        "KNN is computationally expensive because it must calculate the distance to every single point in the dataset for every prediction.\n",
        "\n",
        "The Complement: PCA reduces the number of mathematical operations required for every distance calculation. Calculating distance across 3 dimensions (after PCA) is significantly faster than calculating it across 50 dimensions, making your model viable for larger datasets or real-time use.\n",
        "\n",
        "3- Noise Reduction and Signal Boosting-\n",
        "Raw data often contains \"noisy\" features that don't help with classification but still confuse KNN's distance calculations.\n",
        "\n",
        "The Complement: PCA identifies the directions with the most variance (the signal) and discards the directions with very little variance (often the noise). This \"cleans\" the data before KNN ever sees it, often leading to higher accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "_9lmUStPjzvd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNeVNVgzjyog",
        "outputId": "99ab6d95-75a9-4c32-ddf4-8496031aa695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without feature scaling: 0.7407407407407407\n",
            "Accuracy with feature scaling   : 0.9629629629629629\n"
          ]
        }
      ],
      "source": [
        "#6  Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaling.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_scaling = knn_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "\n",
        "print(\"Accuracy without feature scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with feature scaling   :\", accuracy_scaling)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_, start=1):\n",
        "    print(f\"PC{i}: {ratio:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sQfBgLfogK6",
        "outputId": "4bfde255-eb7f-40ae-bce1-820b14403143"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "scaler_original = StandardScaler()\n",
        "X_train_scaled = scaler_original.fit_transform(X_train)\n",
        "X_test_scaled = scaler_original.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "\n",
        "print(\"Accuracy on original dataset:\", accuracy_original)\n",
        "print(\"Accuracy on PCA-transformed dataset (2 components):\", accuracy_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebvrz4vWo5Mu",
        "outputId": "69ed5eb2-830c-4a34-b3f9-707706cee196"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original dataset: 0.9629629629629629\n",
            "Accuracy on PCA-transformed dataset (2 components): 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9  Train a KNN Classifier with different distance metrics (euclidean,manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "\n",
        "print(\"Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
        "print(\"Accuracy with Manhattan distance:\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eknlw2YpUeO",
        "outputId": "23494b90-db0f-4b32-a0fa-7bb88c4f2bd8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9629629629629629\n",
            "Accuracy with Manhattan distance: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nirTQDPypyV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "- 1- Using PCA to Reduce Dimensionality (Why & How):\n",
        "Problem in gene expression data\n",
        "-> Thousands of genes (features)\n",
        "-> Very few patient samples\n",
        "-> Leads to:\n",
        "1- Overfitting\n",
        "2- High variance\n",
        "3- Poor generalization\n",
        "\n",
        "Solution: PCA\n",
        "-> PCA transforms correlated gene features into uncorrelated principal components\n",
        "-> Keeps maximum biological variance\n",
        "-> Removes noise and redundancy\n",
        "-> Makes distance-based models (like KNN) reliable\n",
        "\n",
        "2- Deciding How Many Components to Keep:\n",
        "We choose components by:\n",
        "-> Cumulative explained variance\n",
        "-> Common biomedical rule:\n",
        "Keep components explaining 90–95% variance\n",
        "This preserves biological signal while removing noise\n",
        "\n",
        "3- Using KNN After PCA:\n",
        "Why KNN?\n",
        "-> Non-parametric (no strong assumptions)\n",
        "-> Effective after dimensionality reduction\n",
        "-> Works well once noise and redundancy are removed\n",
        "\n",
        "4- Model Evaluation Strategy:-\n",
        "To ensure robustness:\n",
        "-> Train/Test split\n",
        "-> Accuracy score\n",
        "-> (Optionally: confusion matrix, cross-validation)\n",
        "This ensures:\n",
        "-> No data leakage\n",
        "-> Reliable generalization\n",
        "\n",
        "5- Justification to Stakeholders (Medical / Research Teams):-\n",
        "Why this pipeline is robust for biomedical data:\n",
        "-> Reduces overfitting in small-sample, high-dimensional data\n",
        "-> Preserves biological variance\n",
        "-> Improves model interpretability\n",
        "-> Computationally efficient\n",
        "-> Widely used in genomics and clinical ML research\n"
      ],
      "metadata": {
        "id": "mwKZTlrGqBRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code of 10th question:-\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"Original number of features:\", X.shape[1])\n",
        "print(\"Reduced number of features after PCA:\", X_train_pca.shape[1])\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"KNN Accuracy after PCA:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW20KjxTrppC",
        "outputId": "22dcfe31-4106-4bc4-8886-41b099f22417"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 30\n",
            "Reduced number of features after PCA: 10\n",
            "KNN Accuracy after PCA: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGI68lePr62j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}